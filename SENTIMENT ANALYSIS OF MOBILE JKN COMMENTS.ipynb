{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projek Akhir Text Mining",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZtLImNcgyff"
      },
      "source": [
        "#Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwS-FUWWNFb3",
        "outputId": "671a73e4-06b5-4c51-f887-1cfd53980ef3"
      },
      "source": [
        "import re\n",
        "import math\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "!pip install Sastrawi\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from statistics import mean"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.7/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peTjvrVpgwu0"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7TwpaIbOIy8"
      },
      "source": [
        "df = pd.read_excel(\"Data_selected.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZshvYdlP242"
      },
      "source": [
        "#Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpsmQB6aOAH3"
      },
      "source": [
        "class Tokenizer:\n",
        "  \n",
        "  def clean(self,text):\n",
        "      #1. Mengubah kata menjadi huruf kecil\n",
        "      text = text.lower()\n",
        "      #2. Lexical Analysis\n",
        "      text = re.sub(\"-\",\" \", text)\n",
        "      text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
        "      text = [word for word in text if not any(c.isdigit() for c in word)]\n",
        "      #4. Stopword Removal\n",
        "      stopword = stopwords.words('indonesian') #mendeteksi stopwords dalam bahasa indonesia\n",
        "      text = [x for x in text if x not in stopword]\n",
        "      #5. Stemming\n",
        "      factory = StemmerFactory()\n",
        "      stemmer = factory.create_stemmer()\n",
        "      text = [stemmer.stem(word) for word in text]\n",
        "      #6. Menghapus kata yang hanya memiliki 1 huruf\n",
        "      text = [t for t in text if len(t) > 1]\n",
        "      #7. Gabungkan semuanya\n",
        "      text = \" \".join(text)\n",
        "      return text\n",
        "\n",
        " \n",
        "  def tokenize(self, text):\n",
        "      clean = self.clean(text)\n",
        "      stopwords_en = stopwords.words(\"english\")\n",
        "      return [w for w in re.split(\"\\W+\", clean) if not w in stopwords_en]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fft7VjUPzeB"
      },
      "source": [
        "#Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AcVGuEDPGsp"
      },
      "source": [
        "class MultinomialNaiveBayes:\n",
        "\n",
        "    def __init__(self, classes, tokenizer):\n",
        "      self.tokenizer = tokenizer\n",
        "      self.classes = classes\n",
        "      \n",
        "    def group_by_class(self, X, y):\n",
        "      data = dict()\n",
        "      for c in self.classes:\n",
        "        data[c] = X[np.where(y == c)]\n",
        "      return data\n",
        "           \n",
        "    def fit(self, X, y):\n",
        "        self.n_class_items = {}\n",
        "        self.class_priors = {}\n",
        "        self.raw_tf = {}\n",
        "        self.term = set()\n",
        "\n",
        "        n = len(X)\n",
        "        \n",
        "        grouped_data = self.group_by_class(X, y)\n",
        "        \n",
        "        for c, data in grouped_data.items():\n",
        "          self.n_class_items[c] = len(data)\n",
        "          self.class_priors[c] = self.n_class_items[c] / n\n",
        "          self.raw_tf[c] = defaultdict(lambda: 0)\n",
        "          \n",
        "          for text in data:\n",
        "            counts = Counter(self.tokenizer.tokenize(text))\n",
        "            for word, count in counts.items():\n",
        "                if word not in self.term:\n",
        "                    self.term.add(word)\n",
        "\n",
        "                self.raw_tf[c][word] += count\n",
        "                \n",
        "        return self\n",
        "      \n",
        "    def conditional_probability(self, word, text_class):\n",
        "      num = self.raw_tf[text_class][word] + 1\n",
        "      denom = self.n_class_items[text_class] + len(self.term)\n",
        "      return num / denom\n",
        "      \n",
        "    def predict(self, X):\n",
        "        result = []\n",
        "        for text in X:\n",
        "          \n",
        "          class_scores = {c: self.class_priors[c] for c in self.classes}\n",
        "\n",
        "          words = set(self.tokenizer.tokenize(text))\n",
        "          for word in words:\n",
        "              if word not in self.term: continue\n",
        "\n",
        "              for c in self.classes:\n",
        "                \n",
        "                w_given_c = self.conditional_probability(word, c)\n",
        "                class_scores[c] *= w_given_c\n",
        "                \n",
        "          result.append(max(class_scores, key=class_scores.get))\n",
        "\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twAk76zullbb"
      },
      "source": [
        "# Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8jI5a7qSir8"
      },
      "source": [
        "X = df['tweet'].values\n",
        "y = df['label'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS8LLIPCX_MI"
      },
      "source": [
        "def stratifiedKFold(X,y,n_splits=5):\n",
        "  skf = StratifiedKFold(n_splits)\n",
        "  lst_accu_stratified = []\n",
        "  NB = MultinomialNaiveBayes(\n",
        "              classes=np.unique(y), \n",
        "              tokenizer=Tokenizer()\n",
        "            )\n",
        "  for train_index, test_index in skf.split(X,y): \n",
        "      print(\"Train:\", train_index, \"Validation:\", test_index) \n",
        "      X_train, X_test = X[train_index], X[test_index] \n",
        "      y_train, y_test = y[train_index], y[test_index]\n",
        "      NB.fit(X_train, y_train)\n",
        "      y_pred = NB.predict(X_test)\n",
        "      print('Predicted: {}'.format(y_pred))\n",
        "      print('Actual: {}\\n'.format(y_test))\n",
        "      lst_accu_stratified.append(accuracy_score(y_test, y_pred))\n",
        "  return lst_accu_stratified,NB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVWB5LjvaIlZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29049d6b-7400-466a-be75-a04e416ece9c"
      },
      "source": [
        "lst_accu_stratified,NB = stratifiedKFold(X,y)\n",
        "print('\\nList of possible accuracy:', lst_accu_stratified)\n",
        "print('\\nMaximum Accuracy That can be obtained from this model is:',\n",
        "          max(lst_accu_stratified)*100, '%')\n",
        "print('\\nMinimum Accuracy:',\n",
        "          min(lst_accu_stratified)*100, '%')\n",
        "print('\\nOverall Accuracy:',\n",
        "          mean(lst_accu_stratified)*100, '%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: [ 3  4  5  6  7  8  9 10 11 12 13 14 18 19 20 21 22 23 24 25 26 27 28 29] Validation: [ 0  1  2 15 16 17]\n",
            "Predicted: [0, 0, 0, 0, 0, 0]\n",
            "\n",
            "Actual: [1 1 1 0 0 0]\n",
            "\n",
            "Train: [ 0  1  2  6  7  8  9 10 11 12 13 14 15 16 17 21 22 23 24 25 26 27 28 29] Validation: [ 3  4  5 18 19 20]\n",
            "Predicted: [0, 1, 0, 0, 0, 0]\n",
            "\n",
            "Actual: [1 1 1 0 0 0]\n",
            "\n",
            "Train: [ 0  1  2  3  4  5  9 10 11 12 13 14 15 16 17 18 19 20 24 25 26 27 28 29] Validation: [ 6  7  8 21 22 23]\n",
            "Predicted: [0, 1, 0, 0, 0, 0]\n",
            "\n",
            "Actual: [1 1 1 0 0 0]\n",
            "\n",
            "Train: [ 0  1  2  3  4  5  6  7  8 12 13 14 15 16 17 18 19 20 21 22 23 27 28 29] Validation: [ 9 10 11 24 25 26]\n",
            "Predicted: [1, 1, 1, 0, 0, 0]\n",
            "\n",
            "Actual: [1 1 1 0 0 0]\n",
            "\n",
            "Train: [ 0  1  2  3  4  5  6  7  8  9 10 11 15 16 17 18 19 20 21 22 23 24 25 26] Validation: [12 13 14 27 28 29]\n",
            "Predicted: [1, 1, 0, 0, 0, 0]\n",
            "\n",
            "Actual: [1 1 1 0 0 0]\n",
            "\n",
            "\n",
            "List of possible accuracy: [0.5, 0.6666666666666666, 0.6666666666666666, 1.0, 0.8333333333333334]\n",
            "\n",
            "Maximum Accuracy That can be obtained from this model is: 100.0 %\n",
            "\n",
            "Minimum Accuracy: 50.0 %\n",
            "\n",
            "Overall Accuracy: 73.33333333333333 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N64JrPwzrFBH"
      },
      "source": [
        "tes = [\"Terima Kasih Mobile JKN atas layanan nya...??????\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV425A0TpMRQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5296e52f-d3f3-45c4-fcd5-9586ff5152ef"
      },
      "source": [
        "doc_pred = NB.predict(tes)\n",
        "doc_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}